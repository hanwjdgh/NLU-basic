# Latent Dirichlet Allocation (LDA)

LDA는 문서들은 토픽들의 혼합으로 구성되어져 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다고 가정한다. 데이터가 주어지면, LDA는 문서가 생성되던 과정을 역추적한다.

## 1. 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA) 개요

문서1 : 저는 사과랑 바나나를 먹어요

문서2 : 우리는 귀여운 강아지가 좋아요

문서3 : 저의 깜찍하고 귀여운 강아지가 바나나를 먹어요
<br/><br/>
LDA는 각 문서의 토픽 분포와 각 토픽 내의 단어 분포를 추정
<br/>
<각 문서의 토픽 분포>

문서1 : 토픽 A 100%
<br/>
문서2 : 토픽 B 100%
<br/>
문서3 : 토픽 B 60%, 토픽 A 40%

<각 토픽의 단어 분포>

토픽A : 사과 20%, 바나나 40%, 먹어요 40%, 귀여운 0%, 강아지 0%, 깜찍하고 0%, 좋아요 0%
<br/>
토픽B : 사과 0%, 바나나 0%, 먹어요 0%, 귀여운 33%, 강아지 33%, 깜찍하고 16%, 좋아요 16%

LDA는 토픽의 제목을 정해주지 않지만, 이 시점에서 알고리즘의 사용자는 두 토픽이 각각 과일에 대한 토픽과 강아지에 대한 토픽이라고 판단할 수 있다.

## 2. LDA의 가정

LDA는 문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘, 빈도수 기반의 표현 방법인 BoW의 행렬 DTM 또는 TF-IDF 행렬을 입력으로 하는데, 이로부터 알 수 있는 사실은 LDA는 단어의 순서는 신경쓰지 않겠다는 것

각각의 문서는 다음과 같은 과정을 거쳐서 작성되었다고 가정

1. 문서에 사용할 단어의 개수 N을 정한다

2. 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정

    - Ex) 위 예제와 같이 토픽이 2개라고 하였을 때 강아지 토픽을 60%, 과일 토픽을 40%와 같이 선택할 수 있다.

3. 문서에 사용할 각 단어를 (아래와 같이) 정한다.

    1. 토픽 분포에서 토픽 T를 확률적으로 고른다.

    - Ex) 60% 확률로 강아지 토픽을 선택하고, 40% 확률로 과일 토픽을 선택할 수 있다.

    2. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고른다.

    - Ex) 강아지 토픽을 선택하였다면, 33% 확률로 강아지란 단어를 선택할 수 있다. 이제 3)을 반복하면서 문서를 완성한다.

이러한 과정을 통해 문서가 작성되었다는 가정 하에 LDA는 토픽을 뽑아내기 위하여 위 과정을 역으로 추적하는 역공학(reverse engneering)을 수행

## 3. LDA의 수행하기

1. 사용자는 알고리즘에게 토픽의 개수 k를 알려준다.

    - LDA는 토픽의 개수 k를 입력받으면, k개의 토픽이 M개의 전체 문서에 걸쳐 분포되어 있다고 가정

2. 모든 단어를 k개 중 하나의 토픽에 할당

3. 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행

    1. 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어져 있지만, 다른 단어들은 전부 올바른 토픽에 할당되어져 있는 상태라고 가정,  이에 따라 단어 w는 아래의 두 가지 기준에 따라서 토픽이 재할당

    - p(topic t | document d) : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율

    - p(word w | topic t) : 단어 w를 갖고 있는 모든 문서들 중 토픽 t가 할당된 비율

## 4. LDA와 LSA의 차이

LSA : DTM을 차원 축소 하여 축소 차원에서 근접 단어들을 토픽으로 묶는다. <br/>
LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출한다.

ex) test1.py, test2.py
