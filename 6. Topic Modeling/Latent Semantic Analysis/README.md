# Latent Semantic Analysis (LSA)

LSA는 정확히는 토픽 모델링을 위해 최적화 된 알고리즘은 아니지만, 토픽 모델링이라는 분야에 아이디어를 제공한 알고리즘이라고 볼 수 있다.

BoW에 기반한 DTM이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이기 때문에 단어의 의미를 고려하지 못한다는 단점이 있었습니다. 이를 위한 대안으로 DTM의 잠재된의미를 이끌어내는 방법으로 잠재 의미 분석(Latent Semantic Analysis, LSA)이라는 방법이 있습니다. 잠재 의미 분석이라 부르기도 한다.

이 방법을 이해하기 위해서는 선형대수학의 특이값 분해(Singular Value Decomposition, SVD)를 이해할 필요가 있다.

## 1. 특이값 분해(Singular Value Decomposition, SVD)

실수 벡터 공간에 한정하여 내용을 설명함을 명시, SVD란 A가 m × n 행렬일 때, 다음과 같이 3개의 행렬의 곱으로 분해하는 것

A=UΣV<sup>T</sup>

여기서 각 3개의 행렬은 다음과 같은 조건을 만족합니다.

U : m × m 직교행렬 (AA<sup>T</sup>=U(ΣΣ<sup>T</sup>)U<sup>T</sup>)

V : n × n 직교행렬 (A<sup>T</sup>A=V(Σ<sup>T</sup>Σ)V<sup>T</sup>)

Σ : m × n 직사각 대각행렬 (내림차순)

1. 전치 행렬(Transposed Matrix)
2. 단위 행렬(Identity Matrix)
3. 역행렬(Inverse Matrix)
4. 직교 행렬(Orthogonal matrix)
5. 대각 행렬(Diagonal matrix)

## 2. 절단된 SVD(Truncated SVD)

위에서 설명한 SVD를 풀 SVD(full SVD)라고 합니다. 하지만 LSA의 경우 풀 SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD(truncated SVD)를 사용

절단된 SVD는 대각 행렬 Σ의 원소의 값 중에서 상위값 t개만,  U행렬과 V행렬의 t열만 남긴다. 이렇게 되면 값의 손실이 일어나므로 기존의 행렬 A를 복구 할 수 없다. 여기서 t는 우리가 찾고자하는 토픽의 수를 반영한 하이퍼파라미터 값이다.

t를 크게 잡으면 기존의 행렬 A로부터 다양한 의미를 가져갈 수 있지만, t를 작게 잡아야만 데이터의 차원을 줄여 풀 SVD를 하였을 때보다 직관적으로 계산 비용이 낮아지는 효과를 얻을 수 있고 상대적으로 중요하지 않은 정보를 삭제하는 효과를 갖고 있다.

이는 설명력이 낮은 정보를 삭제하고 설명력이 높은 정보를 남긴다는 의미를 갖고 있어 기존의 행렬에서는 드러나지 않았던 심층적인 의미를 확인할 수 있게 해준다.

## 3. 잠재 의미 분석(Latent Semantic Analysis, LSA)

기존의 DTM이나 DTM에 단어의 중요도에 따른 가중치를 주었던 TF-IDF 행렬은 단어의 의미를 전혀 고려하지 못한다는 단점을 갖고 있다. LSA는 기본적으로 DTM이나 TF-IDF 행렬에 절단된 SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어낸다는 아이디어를 갖고 있다.

ex) test1.py, test2.py

## 4. LSA의 장단점(Pros and Cons of LSA)

LSA는 쉽고 빠르게 구현이 가능할 뿐만 아니라 단어의 잠재적인 의미를 이끌어낼 수 있어 문서의 유사도 계산 등에서 좋은 성능을 보여준다는 장점을 갖고 있다. 하지만 SVD의 특성상 이미 계산된 LSA에 새로운 데이터를 추가하여 계산하려고하면 보통 처음부터 다시 계산해야 한다. 즉, 새로운 정보에 대해 업데이트가 어렵다. 이는 최근 LSA 대신 Word2Vec 등 단어의 의미를 벡터화할 수 있는 또 다른 방법론인 인공 신경망 기반의 방법론이 각광받는 이유이기도 합니다
