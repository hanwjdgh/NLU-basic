# How to Learn Deep Learning

## 1. 순전파(Foward Propagation)

활성화 함수, 은닉층의 수, 각 은닉층의 뉴런 수 등 딥 러닝 모델을 설계하고나면 입력값은 입력층, 은닉층을 지나면서 각 층에서의 가중치와 함께 연산되며 출력층으로 향하게 되고 출력층에서 모든 연산을 마친 예측값이 나오게 된다. 이와 같이 입력층에서 출력층 방향으로 예측값의 연산이 진행되는 과정을 순전파라고 한다.

## 2. 손실 함수(Loss function)

손실 함수는 실제값과 예측값의 차이를 수치화해주는 함수, 오차가 클 수록 손실 함수의 값은 크고 오차가 작을 수록 손실 함수의 값은 작아진다.

회귀에서는 평균 제곱 오차, 분류 문제에서는 크로스 엔트로피를 주로 손실 함수로 사용, 손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치 W와 편향 b를 찾아가는 것이딥 러닝의 학습 과정이므로 손실 함수의 선정은 매우 중요

- MSE(Mean Squared Error, MSE)

    오차 제곱 평균을 의미합니다. 연속형 변수를 예측할 때 사용

- 크로스 엔트로피(Cross-Entropy)

    낮은 확률로 예측해서 맞추거나, 높은 확률로 예측해서 틀리는 경우 loss가 더 크다.

## 3. 옵티마이저(Optimizer)

손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라진다.

배치는 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양

### 1. 배치 경사 하강법(Batch Gradient Descent)

배치 경사 하강법은 옵티마이저 중 하나로 오차(loss)를 구할 때 전체 데이터를 고려, 한 번의 에포크에 모든 매개변수 업데이트를 단 한 번 수행, 시간이 오래 걸리며, 메모리를 크게 요구한다는 단점이 있으나 글로벌 미니멈을 찾을 수 있다는 장점

```
model.fit(X_train, y_train, batch_size=len(trainX))
```

### 2. 확률적 경사 하강법(Stochastic Gradient Descent, SGD)

매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법, 매개변수의 변경폭이 불안정하고, 때로는 배치 경사 하강법보다 정확도가 낮을 수도 있지만 속도만큼은 배치 경사 하강법보다 빠르다는 장점

```
model.fit(X_train, y_train, batch_size=1)
```

### 3. 미니 배치 경사 하강법(Mini-Batch Gradient Descent)

정해진 양에 대해서만 계산하여 매개 변수의 값을 조정하는 경사 하강법, 미니 배치 경사 하강법은 전체 데이터를 계산하는 것보다 빠르며, SGD보다 안정적이라는 장점

```
model.fit(X_train, y_train, batch_size=32)
```

### 4. 모멘텀(Momentum)

로컬 미니멈에 도달하였을 때, 기울기가 0이라서 기존의 경사 하강법이라면 이를 글로벌 미니멈으로 잘못 인식하여 계산이 끝났을 상황이라도 모멘텀. 즉, 관성의 힘을 빌리면 값이 조절되면서 로컬 미니멈에서 탈출하는 효과를 얻을 수도 있다.

```
keras.optimizers.SGD(lr = 0.01, momentum= 0.9)
```

### 5. 아다그라드(Adagrad)

각 매개변수에 서로 다른 학습률을 적용, 변화가 많은 매개변수는 학습률이 작게 설정되고 변화가 적은 매개변수는 학습률을 높게 설정

```
keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)
```

### 6. 알엠에스프롭(RMSprop)

아다그라드는 학습을 계속 진행한 경우에는, 나중에 가서는 학습률이 지나치게 떨어진다는 단점이 있는데 이를 다른 수식으로 대체하여 이러한 단점을 개선

```
keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)
```

### 7. 아담(Adam)

아담은 알엠에스프롭과 모멘텀 두 가지를 합친 듯한 방법으로, 방향과 학습률 두 가지를 모두 잡기 위한 방법

```
keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
```

## 4. 에포크와 배치 크기와 이터레이션(Epochs and Batch size and Iteration)

기계는 실제값과 예측값의 오차로부터 옵티마이저를 통해서 가중치를 업데이트하는데 머신러닝에서는 이러한 과정을 학습이라고 한다.

### 1. 에포크(Epoch)

전체 데이터에 대해서 순전파와 역전파가 끝난 상태

### 2. 배치 크기(Batch size)

몇 개의 데이터 단위로 매개변수를 업데이트 하는지

### 3. 이터레이션(Iteration)

한 번의 에포크를 끝내기 위해서 필요한 배치의 수

## 5. 과적합을 막는 방법

조기 종료(Early Stopping), 드롭아웃(DropOut), 정규화(Regularization)
