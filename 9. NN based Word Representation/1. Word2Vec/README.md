# Word2Vec

원 핫 인코딩으로 단어를 표현하면 벡터에 단어의 의미가 들어가있지 않기 때문에 원-핫 인코딩은 단어 간 유사성을 계산할 수 없다는 단점이 있다. 의미가 담겨있지 않은 벡터들을 비교하려고 해도 비교할 수는 없다. 이를 해결하기위해, 단어 간 유사성을 고려하기 위해서는 단어의 의미를 벡터화된다. 그리고 이를 가능하게 하는 방법이 워드투벡터(Word2Vec)라고 한다.

NNLM보다 배는 빠른 학습 속도를 가진다.

NNLM : (n × m) + (n × m × h) + (h × V)

Word2Vec : (n × m) + (m × log(V))

## 1. 희소 표현(Sparse Represnetation)

원-핫 인코딩을 통해서 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법

## 2. 분산 표현(Distributed Represnetation)

단어의 '의미'를 다차원 공간에 벡터화하는 방법

## 3. CBOW(Continuous Bag of Words)

CBOW는 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법, 중심 단어(center word)라고 하고, 예측에 사용되는 단어들을 주변 단어(context word)라고 한다.

중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지를 결정했다면 이 범위를 윈도우(window)

윈도우 크기를 정했다면, 윈도우를 계속 움직여서 주변 단어와 중심 단어 선택을 바꿔가며 학습을 위한 데이터 셋을 만들 수 있는데, 이 방법을 슬라이딩 윈도우(sliding window)

## 4. Skip-Gram

중간에 있는 단어로 주변 단어들을 예측하는 방법, 반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있다.

## 5. 네거티브 샘플링(Negative Sampling)

Word2Vec 모델에는 한 가지 문제점은 단어가 중심 단어나 주변 단어와 전혀 상관없는 단어라도 오차를 구하고 모든 단어에 대한 임베딩을 조정한다는 것이다.

주변 단어들을 긍정(positive)으로 두고 랜덤으로 샘플링 된 단어들을 부정(negative)으로 둔 다음에 이진 분류 문제를 수행하는 방법

ex) test1.py

```
model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)
```

Word2Vec()의 인자

- size : 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.
- window : 컨텍스트 윈도우 크기
- min_count : 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)
- workers : 학습을 위한 프로세스 수
- sg : 0은 CBOW, 1은 Skip-gram.
